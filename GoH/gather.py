# -*- coding: utf-8 -*-import osimport sysfrom bs4 import BeautifulSoupimport requestsimport redef get_html_page(url):    """Uses the requests library to get HTML content from URL    Args:        url (str): URL of the page to be scraped.    Returns:        str : HTML text content of the page.    """        r = requests.get(url)    return r.textdef get_local_corpus_data(directory, title):    """Function particular to SDA corpus. Generates a list of local files for each     periodical title that are stored in a given local directory.    Args:        directory (str): Path to local directory where already downloaded files are stored.        title (str): Abbreviation for a particular periodical title.    Returns:        list : List of local files for that periodical title.    """    wd = os.listdir(directory)    listing = []    for each in wd:        if each.startswith(title):            if not each.startswith('.'):                _id = each                listing.append(_id[:-4])    return listingdef filename_from_html(content):    """Uses `Beautiful Soup <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>`_     to extract the PDF ids from the HTML page.     This script is customized to the structure of the archive pages at    http://documents.adventistarchives.org/Periodicals/Forms/AllFolders.aspx.    Args:        content (str): Content is retrieved from a URL using the `get_html_page`             function.    Returns:        list: List of PDF ids extracted from the HTML.    """    soup = BeautifulSoup(content, "lxml")    buttons = soup.find_all('td', class_="ms-vb-title")    pdfIDArray = []    for each in buttons:        links = each.find('a')        pdfID = links.get_text()        pdfIDArray.append(pdfID)    return pdfIDArraydef generate_url(last_title, title):    """Function generates a new URL for iterating through the periodical titles.         Args:        last_title (str): check this - believe this is last pdf ID to be downloaded.        title (str): periodical title        Returns        str: URL for the next web page of records.    """    return "http://documents.adventistarchives.org/Periodicals/Forms/AllItems.aspx?Paged=TRUE&p_SortBehavior=0&p_FileLeafRef={}%2epdf&RootFolder=%2fPeriodicals%2f{}".format(last_title, title)def check_pagination(content):    """Uses Beautiful Soup to see if initial query returned paginated results.    Args:         content (str): text from html page.    Returns:        binary : true if results are paginated, false if single page of results.    """    soup = BeautifulSoup(content, "lxml")    if soup.find_all('td', class_="ms-paging"):        return True    else:        print("No pagination available on page")        return Falsedef check_last_record(pdfIDArray, max_year=1921):    """Checks year of last record on page of results.    Args:        pdfIDArray (list): List of PDF ids from page        max_year (int): Maximum (+1) of year range under study.    Returns:        binary : True if less than max_year. False if greater than max_year    """    last = pdfIDArray[-1]    split_title = last.split('-')    title_date = split_title[0]    date = re.findall(r'[0-9]+', title_date)    year = date[0][:4]    if int(year) < 1921:        return True    else:        return Falsedef check_year(pdfID, max_year=1921):    """Uses regex to check the year from the PDF filename.    Args:        pdfID (str): The filename of the PDF object, formatted as             PREFIXYYYYMMDD-V00-00        max_year (int): Max value of year + 1    Returns:        binary : Returns True or False depending if year is less that 1921.    """    split_title = pdfID.split('-')    title_date = split_title[0]    date = re.findall(r'[0-9]+', title_date)    year = date[0][:4]    if int(year) < max_year:        return True    else:        return Falsedef get_next_url(content, file_list, title):    """Use for scrapping paginated results. Constructs next URL if page is less than upper limit.        Args:        content (str): Unclear.        file_list (list): List of files to scrape        title (str):     Returns:        Str or Binary : String with next URL to scrape or False if past chronological window.    """    if check_last_record(file_list):        last_title = file_list[-1]        new_url = generate_url(last_title, title)        return (new_url)    else:        print("Last record ({}) is after 1920".format(last_title))        return Falsedef process_url(url, pdfIDs, title):    """Compilation function for downloading documents from the Adventist Digital Archives.    Uses    - :meth:`GoH.gather.get_html_page`    - :meth:`GoH.gather.filename_from_html`    - :meth:`GoH.gather.check_pagination`    - :meth:`GoH.gather.check_last_record`    - :meth:`GoH.gather.get_next_url`    Args:         url (str): base URL for the webscrapping query        pdfIDs (list): empty list of ids        title (str): Abbreviation for the periodical title of interest.    Returns:        list: List of found pdfIDs    """    print("Processing {}".format(url))    content = get_html_page(url)    file_list = filename_from_html(content)    pdfIDs.append(file_list)    if check_pagination(content) and check_last_record(file_list):        url = get_next_url(content, file_list, title)        process_url(url, pdfIDs, title)    else:        print("Completed all pages in range.")        return pdfIDs